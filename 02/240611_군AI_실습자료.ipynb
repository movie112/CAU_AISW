{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOmvWbFlczGEgYTSOWh5Pjf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["- 240611(í™”) ì¤‘ì•™ëŒ€í•™êµ êµ° ì¥ë³‘ AISW ì—­ëŸ‰ê°•í™”: ê³ ê¸‰ìì—°ì–´ì²˜ë¦¬ ì‹¤ìŠµ ìë£Œì…ë‹ˆë‹¤.\n","- ë³¸ ë‚´ìš©ì€ IIPL (Intelligent Information Processing Lab) ì†Œì† ì„ì‚¬ê³¼ì • ê¹€ì˜í™” ì¡°êµê°€ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\n","\n","---\n","## 02\n","- RLHF\n","\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1JMjzvX0saRUnRHMeEBmUJOTjpSgXu92u\" width=\"600\">\n","\n","- Step 1. Collect demonstration data, and train a supervised policy.\n","\n","- Step 2. Collect comparison data, and train a reward model.\n","\n","\n","- Step 3. Optimize a policy against the reward model using reinforcement learning.\n","\n","---\n","[ì°¸ì¡°](https://github.com/airobotlab)"],"metadata":{"id":"jhHIfUY5SXAO"}},{"cell_type":"markdown","source":["## Setting"],"metadata":{"id":"mRuOXjiVfCW5"}},{"cell_type":"code","source":["!pip uninstall torch -y\n","!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n","\n","# for transformers, ìµœì‹ ë²„ì „ì€ ì—ëŸ¬ë°œìƒ\n","!pip install transformers==4.35.2\n","!pip install accelerate==0.24.1\n","\n","# for ColossalAI\n","!pip install colossalai==0.2.7\n","\n","# setup data\n","!git clone https://github.com/movie112/CAU_AISW.git\n","!mv CAU_AISW/02/kochatgpt_dataset .\n","\n","%cd CAU_AISW/02/colossalai_ChatGPT_230319/\n","!pip install .\n","%cd ../../../\n","\n","!pip install openai\n","!pip install langchain==0.0.113\n","!pip install pandas>=1.4.1"],"metadata":{"collapsed":true,"id":"8kdavtd8Uobe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Step 1. Collect demonstration data, and train a supervised policy.\n","- ì§ˆë¬¸(prompt)ì— ëŒ€í•´ labelerê°€ ë‹µí•œ ë°ì´í„°ì…‹ í™œìš©(ì§ˆë¬¸-ë‹µë³€ ìŒ)\n","- ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ì„ ì˜í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ supervised fine tuning\n","- ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ ìƒì„±í•˜ëŠ” ëª¨ë¸ -> ì§ˆë¬¸ì— ì˜ ëŒ€ë‹µí•˜ëŠ” ëª¨ë¸\n","  - ë°ì´í„°ì…‹ ì˜ˆì‹œ\n","```json\n","[\n","    {\n","        \"prompt\": \"\",\n","        \"completion\": \"\"\n","    }, ...\n","]\n","```"],"metadata":{"id":"c32u-_RqUPYu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hCZcUloMaP_"},"outputs":[],"source":["# import\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.optim import Adam\n","from datasets import load_dataset\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n","from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","import pandas as pd\n","import argparse\n","from copy import deepcopy\n","import logging\n","import json\n","from dataclasses import dataclass"]},{"cell_type":"code","source":["def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n","    state_dict = trainer.model.state_dict()\n","    if trainer.args.should_save:\n","        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n","        del state_dict\n","        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"],"metadata":{"id":"BXddm2xziryB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--data_path_1_SFT', type=str, default='./kochatgpt_dataset/kochatgpt_1_SFT.jsonl')\n","parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--max_epochs', type=int, default=2)\n","parser.add_argument('--train_batch_size', type=int, default=8)\n","parser.add_argument('--output_dir', type=str, default='./output_1_SFT')\n","\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n","\n","print(args)"],"metadata":{"id":"Y9KUloNrXDkJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### model example"],"metadata":{"id":"vebzw1G9knHQ"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel\n","from transformers import PreTrainedTokenizerFast\n","\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n","                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n","                                                    pad_token='<pad>', mask_token='<mask>')\n","print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\"))"],"metadata":{"id":"vtSXEDxEXEsN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","gen_ids = model.generate(input_ids,\n","                         max_length=128,\n","                         repetition_penalty=2.0,\n","                         pad_token_id=tokenizer.pad_token_id,\n","                         eos_token_id=tokenizer.eos_token_id,\n","                         bos_token_id=tokenizer.bos_token_id,\n","                         use_cache=True)\n","generated = tokenizer.decode(gen_ids[0])\n","print(generated)"],"metadata":{"id":"AUQcvzyLlEVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","generation_args = dict(\n","    num_beams=4,\n","    repetition_penalty=2.0,\n","    no_repeat_ngram_size=4,\n","    eos_token_id=375, # \\n\n","    max_new_tokens=64,\n","    do_sample=True,\n","    top_k=50,\n","    early_stopping=True\n",")\n","generator(\n","    [\"0: **ëŠ” ê²Œì„ ì¢‹ì•„í•˜ë‹ˆ\\n 1 :\",\n","    \"0: ì–´ì œ ê°•ë‚¨ì—ì„œ ì‚´ì¸ì‚¬ê±´ ë‚¬ëŒ€ ã…œã…œ ë„ˆë¬´ ë¬´ì„œì›Œ\\n 1: í— ì™œ? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?\\n 0: ì‚¬ì§„ë³´ë‹ˆê¹Œ ë§‰ í”¼í˜ë¦¬ëŠ” ì‚¬ëŒìˆê³  ê²½ì°°ë“¤ì´ ë– ì„œ ì œì••í•˜ê³  ë‚œë¦¬ë„ ì•„ë‹ˆì—ˆë‹¤ë˜ë°??\\n1 :\",\n","    \"0: ìê¸°ì•¼ ì–´ì œëŠ” ë‚˜í•œí…Œ ì™œ ê·¸ë¬ì–´?\\n 1: ë­” ì¼ ìˆì—ˆì–´?\\n 0: ì–´ë–»ê²Œ ë‚˜í•œí…Œ ë§ë„ ì—†ì´ ê·¸ëŸ´ ìˆ˜ ìˆì–´? ë‚˜ ì§„ì§œ ì‹¤ë§í–ˆì–´\\n 1: \"],\n","    **generation_args\n",")"],"metadata":{"id":"j6bXOrRxlHaX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### .."],"metadata":{"id":"QdZu2zTCkuEk"}},{"cell_type":"code","source":["# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"</s>\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n","        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n","        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n","    ),\n","    \"prompt_no_input\": (\n","        \"Below is an instruction that describes a task.\\n\"\n","        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n","        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n","    ),\n","}"],"metadata":{"id":"godKN8qtXHGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## ëª¨ë¸ ì¤€ë¹„\n","model = AutoModelForCausalLM.from_pretrained(args.model_name)\n","tokenizer = transformers.AutoTokenizer.from_pretrained(\n","    args.model_name,\n","    padding_side=\"right\",\n","    model_max_length=512,\n",")\n","tokenizer.add_special_tokens(\n","    {\n","        \"eos_token\": DEFAULT_EOS_TOKEN,\n","        \"bos_token\": DEFAULT_BOS_TOKEN,\n","        \"unk_token\": DEFAULT_UNK_TOKEN,\n","        \"pad_token\": DEFAULT_PAD_TOKEN,\n","    }\n",")\n","\n","# print(tokenizer)"],"metadata":{"collapsed":true,"id":"U0NAg0gaXHV7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Dict, Sequence\n","\n","class SFTDataset(Dataset):\n","    def __init__(self, data_path:str, tokenizer:transformers.PreTrainedTokenizer, verbose=False):\n","        super(SFTDataset, self).__init__()\n","        logging.warning(\"Loading data...\")\n","\n","        self.tokenizer = tokenizer\n","        self.data_path = data_path\n","        self.verbose = verbose\n","\n","        self.input_ids, self.labels = self.load_data()\n","        logging.warning(\"Loading data done!!: %d\" % len(self.labels))\n","\n","    def load_data(self):\n","        with open(self.data_path, \"r\", encoding='utf-8-sig') as json_file:\n","            data = json.load(json_file)\n","\n","        if self.verbose:\n","            print('## data check ##')\n","            print(data[0])\n","\n","        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n","\n","        sources = [prompt_input.format_map(d) if d.get('input', \"\") else prompt_no_input.format_map(d) for d in data]\n","        targets = [f\"{d['completion']}{self.tokenizer.eos_token}\" for d in data]\n","\n","        if self.verbose:\n","            idx = 0\n","            print(sources[idx])\n","            print(targets[idx])\n","            print(\"Tokenizing inputs... This may take some time...\")\n","\n","        examples = [s + t for s, t in zip(sources, targets)]\n","\n","        sources_tokenized = self._tokenize_fn(sources)\n","        examples_tokenized = self._tokenize_fn(examples)\n","\n","        input_ids = examples_tokenized[\"input_ids\"]\n","        labels = deepcopy(input_ids)\n","        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n","            label[:source_len] = IGNORE_INDEX\n","\n","        return input_ids, labels\n","\n","    def _tokenize_fn(self, texts: Sequence[str]) -> Dict:\n","        tokenized_list = [\n","            self.tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                padding=\"longest\",\n","                max_length=self.tokenizer.model_max_length,\n","                truncation=True,\n","            )\n","            for text in texts\n","        ]\n","        input_ids = [tokenized.input_ids[0] for tokenized in tokenized_list]\n","        input_ids_lens = [tokenized.input_ids.ne(self.tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list]\n","        return dict(input_ids=input_ids, input_ids_lens=input_ids_lens)\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n","        return dict(input_ids=self.input_ids[idx], labels=self.labels[idx])\n","\n","@dataclass\n","class DataCollatorForSupervisedDataset:\n","    tokenizer: transformers.PreTrainedTokenizer\n","\n","    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n","        input_ids, labels = zip(*[(instance['input_ids'], instance['labels']) for instance in instances])\n","        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n","        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n","        return dict(\n","            input_ids=input_ids,\n","            labels=labels,\n","            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n","        )\n","train_dataset = SFTDataset(data_path=args.data_path_1_SFT, tokenizer=tokenizer, verbose=True)\n","eval_dataset = None\n","data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"],"metadata":{"id":"R2K9lIqrZrDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## í•™ìŠµ (10min)\n","training_args = TrainingArguments(\n","    output_dir=\"./test\", #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=1, # number of training epochs\n","    per_device_train_batch_size=4, # batch size for training\n","    per_device_eval_batch_size=4,  # batch size for evaluation\n","    eval_steps = 3, # Number of update steps between two evaluations.\n","    save_steps=500, # after # steps model is saved\n","    warmup_steps=5,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    )\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n",")\n","\n","trainer.train()\n","trainer.save_state()\n","safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"],"metadata":{"id":"D7tV7lClXRyv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## í…ŒìŠ¤íŠ¸\n","generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n","\n","generation_args = dict(\n","    num_beams=4,\n","    repetition_penalty=2.0,\n","    no_repeat_ngram_size=4,\n","    eos_token_id=375, # \\n\n","    max_new_tokens=64,\n","    do_sample=True,\n","    top_k=50,\n","    early_stopping=True\n",")\n","\n","list_prompt = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n","               'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n","               'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n","               'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n","list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n","\n","list_result = generator(list_prompt, **generation_args)\n","for prompt, result in zip(list_prompt, list_result):\n","    print(('#'*70))\n","    print(('completion: %s'%(result[0]['generated_text'])))"],"metadata":{"id":"Q2823KGWXTDB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2. Collect comparison data, and train a reward model.\n","<img src=\"https://drive.google.com/uc?export=view&id=1JMjzvX0saRUnRHMeEBmUJOTjpSgXu92u\" width=\"600\">\n","\n","- Human feedback(ë™ì¼ ì§ˆë¬¸ì— ëŒ€í•´ AIëª¨ë¸ì´ ìƒì„±í•œ ê¸€ë“¤ì„ ranking) ë°ì´í„°ì…‹ í™œìš©\n","- Human preferenceë¥¼ ë°˜ì˜í•œ reward model í•™ìŠµ\n","  - dataset example\n","  ```json\n","  [\n","    {\n","        \"prompt\": \"\",\n","        \"completion_1\": \"\",\n","        \"completion_2\": \"\",\n","        \"completion_3\": \"\",\n","        \"ranking\": [1, 0, 2]\n","    }, ...\n","]\n","```"],"metadata":{"id":"RI2V4oJfaYWq"}},{"cell_type":"code","source":["# import\n","import loralib as lora\n","torch.cuda.empty_cache()\n","from chatgpt.dataset import RewardDataset\n","from chatgpt.models.base import RewardModel\n","from chatgpt.models.bloom import BLOOMRM\n","from chatgpt.models.gpt import GPTRM\n","from chatgpt.models.opt import OPTRM\n","from chatgpt.trainer import RewardModelTrainer\n","from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","from colossalai.nn.optimizer import HybridAdam\n","\n","\n","# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n","        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n","        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n","    ),\n","    \"prompt_no_input\": (\n","        \"Below is an instruction that describes a task.\\n\"\n","        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n","        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n","    ),\n","}"],"metadata":{"id":"_TMf2bQKXXT8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n","parser.add_argument('--data_path_2_RM', type=str, default='./kochatgpt_dataset/kochatgpt_2_RM.jsonl', help='https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompts.csv')\n","parser.add_argument('--strategy',\n","                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n","                    default='naive')\n","parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--pretrain', type=str, default=None)\n","parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n","parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n","parser.add_argument('--max_epochs', type=int, default=10)\n","parser.add_argument('--batch_size', type=int, default=4)\n","parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n","parser.add_argument('--max_len', type=int, default=512)  # wygo ì¶”ê°€\n","\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.max_epochs = 3\n","args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n","args.verbose = True\n","\n","print(args)\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)"],"metadata":{"id":"ClKNKFJibzzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure strategy\n","if args.strategy == 'naive':\n","    strategy = NaiveStrategy()\n","elif args.strategy == 'ddp':\n","    strategy = DDPStrategy()\n","elif args.strategy == 'colossalai_gemini':\n","    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n","elif args.strategy == 'colossalai_zero2':\n","    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n","else:\n","    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"],"metadata":{"id":"SgTD-gfmb0EO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional\n","from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n","from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n","from chatgpt.models.base import RewardModel\n","\n","class GPTRM_custom(RewardModel):\n","    \"\"\"\n","    GPT Reward model.\n","    Args:\n","        pretrained (str): Pretrained model name or path.\n","        config (GPT2Config): Model config.\n","        checkpoint (bool): Enable gradient checkpointing.\n","        lora_rank (int): Rank of the low-rank approximation.\n","        lora_train_bias (str): LoRA bias training mode.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 pretrained: Optional[str] = None,\n","                 config: Optional[GPT2Config] = None,\n","                 checkpoint: bool = False,\n","                 lora_rank: int = 0,\n","                 lora_train_bias: str = 'none',\n","                 tokenizer=None) -> None:\n","        if pretrained is not None:\n","            model = GPT2Model.from_pretrained(pretrained)\n","            model.resize_token_embeddings(len(tokenizer))  # wygo ì¶”ê°€!!!\n","        elif config is not None:\n","            model = GPT2Model(config)\n","        else:\n","            model = GPT2Model(GPT2Config())\n","        if checkpoint:\n","            model.gradient_checkpointing_enable()\n","\n","        value_head = nn.Linear(model.config.n_embd, 1)\n","        super().__init__(model, value_head, lora_rank, lora_train_bias)\n","\n","        if pretrained is not None:\n","            self.model = model\n","            self.pretrained = pretrained\n","\n","    def save_pretrained(self, dir):\n","        if self.pretrained is not None:\n","            self.model.save_pretrained(dir)"],"metadata":{"id":"bU2F_IEfb16d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure model, tokenizer\n","with strategy.model_init_context():\n","    # load pretrained gpt2\n","    if args.model == 'gpt2':\n","        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n","        tokenizer.add_special_tokens(\n","            {\n","                \"eos_token\": DEFAULT_EOS_TOKEN,\n","                \"bos_token\": DEFAULT_BOS_TOKEN,\n","                \"unk_token\": DEFAULT_UNK_TOKEN,\n","            }\n","        )\n","        tokenizer.pad_token = tokenizer.eos_token\n","        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n","\n","    elif args.model == 'bloom':\n","        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n","        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n","\n","    elif args.model == 'opt':\n","        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n","        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n","\n","    else:\n","        raise ValueError(f'Unsupported model \"{args.model}\"')"],"metadata":{"id":"CxjPy_ffb23J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make ranking data to chosen, rejetced data\n","with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n","    list_data_dict = json.load(json_file)\n","    if args.verbose:\n","        print('## data check ##')\n","        print((list_data_dict[0]))\n","\n","total_data_ranking2chosen = []\n","for tmp in list_data_dict:\n","    one_data_ranking2chosen = []\n","\n","    # data 1) 0 VS 1\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][1]:\n","        data['chosen'] = tmp['completion_0']\n","        data['rejected'] = tmp['completion_1']\n","    else:\n","        data['chosen'] = tmp['completion_1']\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","    # data 2) 0 VS 2\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][2]:\n","        data['chosen'] = tmp['completion_0']\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = tmp['completion_2']\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","    # data 1) 1 VS 2\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][1] < tmp['ranking'][2]:\n","        data['chosen'] = tmp['completion_1']\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = tmp['completion_2']\n","        data['rejected'] = tmp['completion_1']\n","    one_data_ranking2chosen.append(data)\n","\n","\n","    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n","\n","print('before data num: %d'%(len(list_data_dict)))\n","print('after  data num: %d'%(len(total_data_ranking2chosen)))\n","print('data example: \\n%s'%total_data_ranking2chosen[0])\n","print('data example: \\n%s'%total_data_ranking2chosen[1])\n","print('data example: \\n%s'%total_data_ranking2chosen[2])"],"metadata":{"id":"7sXmnZlOb3x-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare for data and dataset\n","import random\n","random.seed(230319)\n","# list_tmp = list(range(10))\n","random.shuffle(total_data_ranking2chosen)\n","print(total_data_ranking2chosen[45])\n","\n","# train_data = total_data_ranking2chosen[:-1000]  # 29000 í•™ìŠµ\n","# eval_data = total_data_ranking2chosen[-1000:0]  # 1000ê°œë§Œ í‰ê°€\n","\n","train_data = total_data_ranking2chosen[:100]  # 29000 í•™ìŠµ\n","eval_data = total_data_ranking2chosen[100:130]  # 1000ê°œë§Œ í‰ê°€\n","\n","\n","train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n","eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n","\n","# check\n","idx = 10\n","print('#'*70)\n","print('## prompt ##')\n","print(train_data[idx]['prompt'])\n","print('#'*70)\n","print('## chosen ##')\n","print(train_data[idx]['chosen'])\n","print('#'*70)\n","print('## rejected ##')\n","print(train_data[idx]['rejected'])"],"metadata":{"id":"4Vtqb8Ojb4rS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure optimizer\n","if args.strategy.startswith('colossalai'):\n","    optim = HybridAdam(model.parameters(), lr=5e-5)\n","else:\n","    optim = Adam(model.parameters(), lr=5e-5)"],"metadata":{"id":"2OGJ2IChb56j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# batch_size here is expected to be C(k,2), k means # response of each prompt\n","# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n","trainer = RewardModelTrainer(model=model,\n","                             strategy=strategy,\n","                             optim=optim,\n","                             train_dataset=train_dataset,\n","                             eval_dataset=eval_dataset,\n","                             batch_size=args.batch_size,\n","                             max_epochs=args.max_epochs)"],"metadata":{"id":"I8Rievxbb6p2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GyIn4FMJb9ZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train!!\n","trainer.fit(use_lora=args.lora_rank)\n","\n","## save\n","# save model checkpoint after fitting on only rank0\n","strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n","# save optimizer checkpoint on all ranks\n","strategy.save_optimizer(optim,\n","                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n","                        only_rank0=False)\n","\n","model.save_pretrained(args.output_dir)  # config.json ìƒì„±"],"metadata":{"id":"c8bCacRtb94r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reward model ì²´í¬\n","def inference_RM(input_text='ì¸ê³µì§€ëŠ¥ì€ ì¸ê³µì§€ëŠ¥ ì…ë‹ˆë‹¤'):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n","        torch.cuda.current_device())\n","    output = model(input_ids)\n","    output_reward = output.cpu().detach().numpy()[0]\n","\n","    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n","    print(output)\n","\n","    return output_reward\n","\n","# input_text = 'í•œêµ­ì€ ëŒ€í•œë¯¼êµ­ ì…ë‹ˆë‹¤'\n","input_text = 'ì¸ê³µì§€ëŠ¥ì€ ì¸ê³µì§€ëŠ¥ ì…ë‹ˆë‹¤'\n","\n","output_reward = inference_RM(input_text=input_text)"],"metadata":{"id":"P10f0Qtpb_q5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Step 3. Optimize a policy against the reward model using reinforcement learning.\n","<img src=\"https://drive.google.com/uc?export=view&id=1JMjzvX0saRUnRHMeEBmUJOTjpSgXu92u\" width=\"600\">\n","\n","- Human feedbackì„ ëª¨ì‚¬í•œ reward modelì˜ ì ìˆ˜ê°€ ë†’ì•„ì§€ë„ë¡ í•™ìŠµ"],"metadata":{"id":"2jgrMak0cCUP"}},{"cell_type":"code","source":["# import\n","torch.cuda.empty_cache()\n","from chatgpt.models.base import RewardModel\n","from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n","from chatgpt.models.gpt import GPTActor, GPTCritic\n","from chatgpt.models.opt import OPTActor, OPTCritic\n","from chatgpt.trainer import PPOTrainer\n","from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n","from torch.optim import Adam\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","\n","from colossalai.nn.optimizer import HybridAdam\n","\n","## clossalAI error í•´ê²°\n","os.environ['RANK'] = '0'\n","os.environ['LOCAL_RANK'] = '0'\n","os.environ['WORLD_SIZE'] = '2'\n","os.environ['MASTER_ADDR'] = '127.0.0.1'\n","os.environ['MASTER_PORT'] = '42043'\n","\n","# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n","        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n","        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n","    ),\n","    \"prompt_no_input\": (\n","        \"Below is an instruction that describes a task.\\n\"\n","        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n","        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n","    ),\n","}"],"metadata":{"id":"IvR8uvoZcJPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--data_path_3_PPO', type=str, default='./kochatgpt_dataset/kochatgpt_3_PPO.jsonl')\n","parser.add_argument('--output_dir', type=str, default='./output_3_PPO')\n","parser.add_argument('--strategy',\n","                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n","                    default='naive')\n","parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--pretrain', type=str, default=None)\n","parser.add_argument('--num_episodes', type=int, default=10)\n","parser.add_argument('--max_timesteps', type=int, default=3)\n","parser.add_argument('--update_timesteps', type=int, default=3)\n","parser.add_argument('--max_epochs', type=int, default=5)\n","parser.add_argument('--train_batch_size', type=int, default=8)\n","parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n","parser.add_argument('--max_length', type=int, default=250)\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.output_dir = './output_3_PPO'\n","args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n","\n","args.pretrain_actor = './output_1_SFT'  # SFT ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n","args.pretrain_critic = './output_2_RM'  # RM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n","# args.pretrain_actor = args.pretrain\n","# args.pretrain_critic = args.pretrain\n","\n","args.num_episodes = 1\n","args.max_epochs   = 1\n","\n","print(args)\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)"],"metadata":{"id":"zZQTezoCdF-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure strategy\n","if args.strategy == 'naive':\n","    strategy = NaiveStrategy()\n","elif args.strategy == 'ddp':\n","    strategy = DDPStrategy()\n","elif args.strategy == 'colossalai_gemini':\n","    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n","elif args.strategy == 'colossalai_zero2':\n","    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n","else:\n","    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"],"metadata":{"id":"jI4VirtndHU3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure model, tokenizer\n","with strategy.model_init_context():\n","    if args.model == 'gpt2':\n","        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","        # tokenizer.pad_token = tokenizer.eos_token\n","        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n","        tokenizer.add_special_tokens(\n","            {\n","                \"eos_token\": DEFAULT_EOS_TOKEN,\n","                \"bos_token\": DEFAULT_BOS_TOKEN,\n","                \"unk_token\": DEFAULT_UNK_TOKEN,\n","            }\n","        )\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    elif args.model == 'bloom':\n","        actor = BLOOMActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        critic = BLOOMCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n","        tokenizer.pad_token = tokenizer.eos_token\n","    elif args.model == 'opt':\n","        actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        critic = OPTCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n","    else:\n","        raise ValueError(f'Unsupported model \"{args.model}\"')\n","\n","    initial_model = deepcopy(actor)\n","    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"],"metadata":{"id":"T47TCyZVdIfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure optimizer\n","if args.strategy.startswith('colossalai'):\n","    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n","    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n","else:\n","    actor_optim = Adam(actor.parameters(), lr=5e-6)\n","    critic_optim = Adam(critic.parameters(), lr=5e-6)"],"metadata":{"id":"zJV34DhHdJ22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setting the models\n","(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n","    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"],"metadata":{"id":"ebH7NYWzdK5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare data\n","with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n","    list_data_dict = json.load(json_file)\n","    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n","\n","def tokenize_fn(texts):\n","    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n","    return {k: v.cuda() for k, v in batch.items()}\n","\n","print(list_prompt)\n","print('\\n\\n\\n')\n","print(tokenize_fn('I want you to act as a linux terminal.'))"],"metadata":{"id":"RE2E3nkJdpx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure trainer\n","trainer = PPOTrainer(strategy,\n","                     actor,\n","                     critic,\n","                     reward_model,\n","                     initial_model,\n","                     actor_optim,\n","                     critic_optim,\n","                     max_epochs=args.max_epochs,\n","                     train_batch_size=args.train_batch_size,\n","                     tokenizer=tokenize_fn,\n","                     max_length=128,\n","                     do_sample=True,\n","                     temperature=1.0,\n","                     top_k=50,\n","                     pad_token_id=tokenizer.pad_token_id,\n","                     eos_token_id=tokenizer.eos_token_id)\n","\n","## train!\n","trainer.fit(list_prompt,  # ì…ë ¥ prompt\n","            num_episodes=args.num_episodes,\n","            max_timesteps=args.max_timesteps,\n","            update_timesteps=args.update_timesteps)\n","\n","## save\n","# save model checkpoint after fitting on only rank0\n","strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n","# save optimizer checkpoint on all ranks\n","strategy.save_optimizer(actor_optim,\n","                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n","                        only_rank0=False)"],"metadata":{"id":"yI4Cos_2dqc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## inference\n","def generation(input_text):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n","        torch.cuda.current_device())\n","    outputs = actor.generate(input_ids,\n","                             max_length=args.max_length,\n","                             do_sample=True,\n","                             top_k=50,\n","                             top_p=0.95,\n","                             num_return_sequences=1)\n","    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n","    print('#' * 70)\n","    print(output)\n","    return output\n","\n","\n","list_prompt = [\n","    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n","    'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n","    'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n","    'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n","\n","list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n","\n","for input_text in list_prompt:\n","    output = generation(input_text)"],"metadata":{"id":"VSzpNjdwdtbo"},"execution_count":null,"outputs":[]}]}
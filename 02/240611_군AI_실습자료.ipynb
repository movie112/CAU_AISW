{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOmvWbFlczGEgYTSOWh5Pjf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["- 240611(화) 중앙대학교 군 장병 AISW 역량강화: 고급자연어처리 실습 자료입니다.\n","- 본 내용은 IIPL (Intelligent Information Processing Lab) 소속 석사과정 김영화 조교가 작성하였습니다.\n","\n","---\n","## 02\n","- RLHF\n","\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1JMjzvX0saRUnRHMeEBmUJOTjpSgXu92u\" width=\"600\">\n","\n","- Step 1. Collect demonstration data, and train a supervised policy.\n","\n","- Step 2. Collect comparison data, and train a reward model.\n","\n","\n","- Step 3. Optimize a policy against the reward model using reinforcement learning.\n","\n","---\n","[참조](https://github.com/airobotlab)"],"metadata":{"id":"jhHIfUY5SXAO"}},{"cell_type":"markdown","source":["## Setting"],"metadata":{"id":"mRuOXjiVfCW5"}},{"cell_type":"code","source":["!pip uninstall torch -y\n","!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n","\n","# for transformers, 최신버전은 에러발생\n","!pip install transformers==4.35.2\n","!pip install accelerate==0.24.1\n","\n","# for ColossalAI\n","!pip install colossalai==0.2.7\n","\n","# setup data\n","!git clone https://github.com/movie112/CAU_AISW.git\n","!mv CAU_AISW/02/kochatgpt_dataset .\n","\n","%cd CAU_AISW/02/colossalai_ChatGPT_230319/\n","!pip install .\n","%cd ../../../\n","\n","!pip install openai\n","!pip install langchain==0.0.113\n","!pip install pandas>=1.4.1"],"metadata":{"collapsed":true,"id":"8kdavtd8Uobe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Step 1. Collect demonstration data, and train a supervised policy.\n","- 질문(prompt)에 대해 labeler가 답한 데이터셋 활용(질문-답변 쌍)\n","- 질문에 대해 답변을 잘하기 위해 모델을 supervised fine tuning\n","- 다음 단어를 잘 생성하는 모델 -> 질문에 잘 대답하는 모델\n","  - 데이터셋 예시\n","```json\n","[\n","    {\n","        \"prompt\": \"\",\n","        \"completion\": \"\"\n","    }, ...\n","]\n","```"],"metadata":{"id":"c32u-_RqUPYu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hCZcUloMaP_"},"outputs":[],"source":["# import\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.optim import Adam\n","from datasets import load_dataset\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n","from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","import pandas as pd\n","import argparse\n","from copy import deepcopy\n","import logging\n","import json\n","from dataclasses import dataclass"]},{"cell_type":"code","source":["def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n","    state_dict = trainer.model.state_dict()\n","    if trainer.args.should_save:\n","        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n","        del state_dict\n","        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"],"metadata":{"id":"BXddm2xziryB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--data_path_1_SFT', type=str, default='./kochatgpt_dataset/kochatgpt_1_SFT.jsonl')\n","parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--max_epochs', type=int, default=2)\n","parser.add_argument('--train_batch_size', type=int, default=8)\n","parser.add_argument('--output_dir', type=str, default='./output_1_SFT')\n","\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n","\n","print(args)"],"metadata":{"id":"Y9KUloNrXDkJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### model example"],"metadata":{"id":"vebzw1G9knHQ"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel\n","from transformers import PreTrainedTokenizerFast\n","\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n","                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n","                                                    pad_token='<pad>', mask_token='<mask>')\n","print(tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\"))"],"metadata":{"id":"vtSXEDxEXEsN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","text = '근육이 커지기 위해서는'\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","gen_ids = model.generate(input_ids,\n","                         max_length=128,\n","                         repetition_penalty=2.0,\n","                         pad_token_id=tokenizer.pad_token_id,\n","                         eos_token_id=tokenizer.eos_token_id,\n","                         bos_token_id=tokenizer.bos_token_id,\n","                         use_cache=True)\n","generated = tokenizer.decode(gen_ids[0])\n","print(generated)"],"metadata":{"id":"AUQcvzyLlEVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","generation_args = dict(\n","    num_beams=4,\n","    repetition_penalty=2.0,\n","    no_repeat_ngram_size=4,\n","    eos_token_id=375, # \\n\n","    max_new_tokens=64,\n","    do_sample=True,\n","    top_k=50,\n","    early_stopping=True\n",")\n","generator(\n","    [\"0: **는 게임 좋아하니\\n 1 :\",\n","    \"0: 어제 강남에서 살인사건 났대 ㅜㅜ 너무 무서워\\n 1: 헐 왜? 무슨 일 있었어?\\n 0: 사진보니까 막 피흘리는 사람있고 경찰들이 떠서 제압하고 난리도 아니었다던데??\\n1 :\",\n","    \"0: 자기야 어제는 나한테 왜 그랬어?\\n 1: 뭔 일 있었어?\\n 0: 어떻게 나한테 말도 없이 그럴 수 있어? 나 진짜 실망했어\\n 1: \"],\n","    **generation_args\n",")"],"metadata":{"id":"j6bXOrRxlHaX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### .."],"metadata":{"id":"QdZu2zTCkuEk"}},{"cell_type":"code","source":["# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"</s>\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n","        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n","    ),\n","    \"prompt_no_input\": (\n","        \"Below is an instruction that describes a task.\\n\"\n","        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n","    ),\n","}"],"metadata":{"id":"godKN8qtXHGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 모델 준비\n","model = AutoModelForCausalLM.from_pretrained(args.model_name)\n","tokenizer = transformers.AutoTokenizer.from_pretrained(\n","    args.model_name,\n","    padding_side=\"right\",\n","    model_max_length=512,\n",")\n","tokenizer.add_special_tokens(\n","    {\n","        \"eos_token\": DEFAULT_EOS_TOKEN,\n","        \"bos_token\": DEFAULT_BOS_TOKEN,\n","        \"unk_token\": DEFAULT_UNK_TOKEN,\n","        \"pad_token\": DEFAULT_PAD_TOKEN,\n","    }\n",")\n","\n","# print(tokenizer)"],"metadata":{"collapsed":true,"id":"U0NAg0gaXHV7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Dict, Sequence\n","\n","class SFTDataset(Dataset):\n","    def __init__(self, data_path:str, tokenizer:transformers.PreTrainedTokenizer, verbose=False):\n","        super(SFTDataset, self).__init__()\n","        logging.warning(\"Loading data...\")\n","\n","        self.tokenizer = tokenizer\n","        self.data_path = data_path\n","        self.verbose = verbose\n","\n","        self.input_ids, self.labels = self.load_data()\n","        logging.warning(\"Loading data done!!: %d\" % len(self.labels))\n","\n","    def load_data(self):\n","        with open(self.data_path, \"r\", encoding='utf-8-sig') as json_file:\n","            data = json.load(json_file)\n","\n","        if self.verbose:\n","            print('## data check ##')\n","            print(data[0])\n","\n","        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n","\n","        sources = [prompt_input.format_map(d) if d.get('input', \"\") else prompt_no_input.format_map(d) for d in data]\n","        targets = [f\"{d['completion']}{self.tokenizer.eos_token}\" for d in data]\n","\n","        if self.verbose:\n","            idx = 0\n","            print(sources[idx])\n","            print(targets[idx])\n","            print(\"Tokenizing inputs... This may take some time...\")\n","\n","        examples = [s + t for s, t in zip(sources, targets)]\n","\n","        sources_tokenized = self._tokenize_fn(sources)\n","        examples_tokenized = self._tokenize_fn(examples)\n","\n","        input_ids = examples_tokenized[\"input_ids\"]\n","        labels = deepcopy(input_ids)\n","        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n","            label[:source_len] = IGNORE_INDEX\n","\n","        return input_ids, labels\n","\n","    def _tokenize_fn(self, texts: Sequence[str]) -> Dict:\n","        tokenized_list = [\n","            self.tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                padding=\"longest\",\n","                max_length=self.tokenizer.model_max_length,\n","                truncation=True,\n","            )\n","            for text in texts\n","        ]\n","        input_ids = [tokenized.input_ids[0] for tokenized in tokenized_list]\n","        input_ids_lens = [tokenized.input_ids.ne(self.tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list]\n","        return dict(input_ids=input_ids, input_ids_lens=input_ids_lens)\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n","        return dict(input_ids=self.input_ids[idx], labels=self.labels[idx])\n","\n","@dataclass\n","class DataCollatorForSupervisedDataset:\n","    tokenizer: transformers.PreTrainedTokenizer\n","\n","    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n","        input_ids, labels = zip(*[(instance['input_ids'], instance['labels']) for instance in instances])\n","        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n","        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n","        return dict(\n","            input_ids=input_ids,\n","            labels=labels,\n","            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n","        )\n","train_dataset = SFTDataset(data_path=args.data_path_1_SFT, tokenizer=tokenizer, verbose=True)\n","eval_dataset = None\n","data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"],"metadata":{"id":"R2K9lIqrZrDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 학습 (10min)\n","training_args = TrainingArguments(\n","    output_dir=\"./test\", #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=1, # number of training epochs\n","    per_device_train_batch_size=4, # batch size for training\n","    per_device_eval_batch_size=4,  # batch size for evaluation\n","    eval_steps = 3, # Number of update steps between two evaluations.\n","    save_steps=500, # after # steps model is saved\n","    warmup_steps=5,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    )\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n",")\n","\n","trainer.train()\n","trainer.save_state()\n","safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"],"metadata":{"id":"D7tV7lClXRyv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 테스트\n","generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n","\n","generation_args = dict(\n","    num_beams=4,\n","    repetition_penalty=2.0,\n","    no_repeat_ngram_size=4,\n","    eos_token_id=375, # \\n\n","    max_new_tokens=64,\n","    do_sample=True,\n","    top_k=50,\n","    early_stopping=True\n",")\n","\n","list_prompt = ['불고기용 고기 한우에요?',\n","               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n","               '시카고 오헤어 국제공항은 어디에 있어',\n","               '오늘 미세먼지 어때?']\n","list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n","\n","list_result = generator(list_prompt, **generation_args)\n","for prompt, result in zip(list_prompt, list_result):\n","    print(('#'*70))\n","    print(('completion: %s'%(result[0]['generated_text'])))"],"metadata":{"id":"Q2823KGWXTDB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2. Collect comparison data, and train a reward model.\n","<img src=\"https://drive.google.com/uc?export=view&id=1JMjzvX0saRUnRHMeEBmUJOTjpSgXu92u\" width=\"600\">\n","\n","- Human feedback(동일 질문에 대해 AI모델이 생성한 글들을 ranking) 데이터셋 활용\n","- Human preference를 반영한 reward model 학습\n","  - dataset example\n","  ```json\n","  [\n","    {\n","        \"prompt\": \"\",\n","        \"completion_1\": \"\",\n","        \"completion_2\": \"\",\n","        \"completion_3\": \"\",\n","        \"ranking\": [1, 0, 2]\n","    }, ...\n","]\n","```"],"metadata":{"id":"RI2V4oJfaYWq"}},{"cell_type":"code","source":["# import\n","import loralib as lora\n","torch.cuda.empty_cache()\n","from chatgpt.dataset import RewardDataset\n","from chatgpt.models.base import RewardModel\n","from chatgpt.models.bloom import BLOOMRM\n","from chatgpt.models.gpt import GPTRM\n","from chatgpt.models.opt import OPTRM\n","from chatgpt.trainer import RewardModelTrainer\n","from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","from colossalai.nn.optimizer import HybridAdam\n","\n","\n","# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n","        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n","    ),\n","    \"prompt_no_input\": (\n","        \"Below is an instruction that describes a task.\\n\"\n","        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n","    ),\n","}"],"metadata":{"id":"_TMf2bQKXXT8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n","parser.add_argument('--data_path_2_RM', type=str, default='./kochatgpt_dataset/kochatgpt_2_RM.jsonl', help='https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompts.csv')\n","parser.add_argument('--strategy',\n","                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n","                    default='naive')\n","parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--pretrain', type=str, default=None)\n","parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n","parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n","parser.add_argument('--max_epochs', type=int, default=10)\n","parser.add_argument('--batch_size', type=int, default=4)\n","parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n","parser.add_argument('--max_len', type=int, default=512)  # wygo 추가\n","\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.max_epochs = 3\n","args.pretrain = 'skt/kogpt2-base-v2'  # pretrained 모델 가져오기\n","args.verbose = True\n","\n","print(args)\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)"],"metadata":{"id":"ClKNKFJibzzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure strategy\n","if args.strategy == 'naive':\n","    strategy = NaiveStrategy()\n","elif args.strategy == 'ddp':\n","    strategy = DDPStrategy()\n","elif args.strategy == 'colossalai_gemini':\n","    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n","elif args.strategy == 'colossalai_zero2':\n","    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n","else:\n","    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"],"metadata":{"id":"SgTD-gfmb0EO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional\n","from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n","from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n","from chatgpt.models.base import RewardModel\n","\n","class GPTRM_custom(RewardModel):\n","    \"\"\"\n","    GPT Reward model.\n","    Args:\n","        pretrained (str): Pretrained model name or path.\n","        config (GPT2Config): Model config.\n","        checkpoint (bool): Enable gradient checkpointing.\n","        lora_rank (int): Rank of the low-rank approximation.\n","        lora_train_bias (str): LoRA bias training mode.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 pretrained: Optional[str] = None,\n","                 config: Optional[GPT2Config] = None,\n","                 checkpoint: bool = False,\n","                 lora_rank: int = 0,\n","                 lora_train_bias: str = 'none',\n","                 tokenizer=None) -> None:\n","        if pretrained is not None:\n","            model = GPT2Model.from_pretrained(pretrained)\n","            model.resize_token_embeddings(len(tokenizer))  # wygo 추가!!!\n","        elif config is not None:\n","            model = GPT2Model(config)\n","        else:\n","            model = GPT2Model(GPT2Config())\n","        if checkpoint:\n","            model.gradient_checkpointing_enable()\n","\n","        value_head = nn.Linear(model.config.n_embd, 1)\n","        super().__init__(model, value_head, lora_rank, lora_train_bias)\n","\n","        if pretrained is not None:\n","            self.model = model\n","            self.pretrained = pretrained\n","\n","    def save_pretrained(self, dir):\n","        if self.pretrained is not None:\n","            self.model.save_pretrained(dir)"],"metadata":{"id":"bU2F_IEfb16d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure model, tokenizer\n","with strategy.model_init_context():\n","    # load pretrained gpt2\n","    if args.model == 'gpt2':\n","        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n","        tokenizer.add_special_tokens(\n","            {\n","                \"eos_token\": DEFAULT_EOS_TOKEN,\n","                \"bos_token\": DEFAULT_BOS_TOKEN,\n","                \"unk_token\": DEFAULT_UNK_TOKEN,\n","            }\n","        )\n","        tokenizer.pad_token = tokenizer.eos_token\n","        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n","\n","    elif args.model == 'bloom':\n","        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n","        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n","\n","    elif args.model == 'opt':\n","        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n","        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n","\n","    else:\n","        raise ValueError(f'Unsupported model \"{args.model}\"')"],"metadata":{"id":"CxjPy_ffb23J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make ranking data to chosen, rejetced data\n","with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n","    list_data_dict = json.load(json_file)\n","    if args.verbose:\n","        print('## data check ##')\n","        print((list_data_dict[0]))\n","\n","total_data_ranking2chosen = []\n","for tmp in list_data_dict:\n","    one_data_ranking2chosen = []\n","\n","    # data 1) 0 VS 1\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][1]:\n","        data['chosen'] = tmp['completion_0']\n","        data['rejected'] = tmp['completion_1']\n","    else:\n","        data['chosen'] = tmp['completion_1']\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","    # data 2) 0 VS 2\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][2]:\n","        data['chosen'] = tmp['completion_0']\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = tmp['completion_2']\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","    # data 1) 1 VS 2\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][1] < tmp['ranking'][2]:\n","        data['chosen'] = tmp['completion_1']\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = tmp['completion_2']\n","        data['rejected'] = tmp['completion_1']\n","    one_data_ranking2chosen.append(data)\n","\n","\n","    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n","\n","print('before data num: %d'%(len(list_data_dict)))\n","print('after  data num: %d'%(len(total_data_ranking2chosen)))\n","print('data example: \\n%s'%total_data_ranking2chosen[0])\n","print('data example: \\n%s'%total_data_ranking2chosen[1])\n","print('data example: \\n%s'%total_data_ranking2chosen[2])"],"metadata":{"id":"7sXmnZlOb3x-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare for data and dataset\n","import random\n","random.seed(230319)\n","# list_tmp = list(range(10))\n","random.shuffle(total_data_ranking2chosen)\n","print(total_data_ranking2chosen[45])\n","\n","# train_data = total_data_ranking2chosen[:-1000]  # 29000 학습\n","# eval_data = total_data_ranking2chosen[-1000:0]  # 1000개만 평가\n","\n","train_data = total_data_ranking2chosen[:100]  # 29000 학습\n","eval_data = total_data_ranking2chosen[100:130]  # 1000개만 평가\n","\n","\n","train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n","eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n","\n","# check\n","idx = 10\n","print('#'*70)\n","print('## prompt ##')\n","print(train_data[idx]['prompt'])\n","print('#'*70)\n","print('## chosen ##')\n","print(train_data[idx]['chosen'])\n","print('#'*70)\n","print('## rejected ##')\n","print(train_data[idx]['rejected'])"],"metadata":{"id":"4Vtqb8Ojb4rS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure optimizer\n","if args.strategy.startswith('colossalai'):\n","    optim = HybridAdam(model.parameters(), lr=5e-5)\n","else:\n","    optim = Adam(model.parameters(), lr=5e-5)"],"metadata":{"id":"2OGJ2IChb56j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# batch_size here is expected to be C(k,2), k means # response of each prompt\n","# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n","trainer = RewardModelTrainer(model=model,\n","                             strategy=strategy,\n","                             optim=optim,\n","                             train_dataset=train_dataset,\n","                             eval_dataset=eval_dataset,\n","                             batch_size=args.batch_size,\n","                             max_epochs=args.max_epochs)"],"metadata":{"id":"I8Rievxbb6p2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GyIn4FMJb9ZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train!!\n","trainer.fit(use_lora=args.lora_rank)\n","\n","## save\n","# save model checkpoint after fitting on only rank0\n","strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n","# save optimizer checkpoint on all ranks\n","strategy.save_optimizer(optim,\n","                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n","                        only_rank0=False)\n","\n","model.save_pretrained(args.output_dir)  # config.json 생성"],"metadata":{"id":"c8bCacRtb94r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reward model 체크\n","def inference_RM(input_text='인공지능은 인공지능 입니다'):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n","        torch.cuda.current_device())\n","    output = model(input_ids)\n","    output_reward = output.cpu().detach().numpy()[0]\n","\n","    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n","    print(output)\n","\n","    return output_reward\n","\n","# input_text = '한국은 대한민국 입니다'\n","input_text = '인공지능은 인공지능 입니다'\n","\n","output_reward = inference_RM(input_text=input_text)"],"metadata":{"id":"P10f0Qtpb_q5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Step 3. Optimize a policy against the reward model using reinforcement learning.\n","<img src=\"https://drive.google.com/uc?export=view&id=1JMjzvX0saRUnRHMeEBmUJOTjpSgXu92u\" width=\"600\">\n","\n","- Human feedback을 모사한 reward model의 점수가 높아지도록 학습"],"metadata":{"id":"2jgrMak0cCUP"}},{"cell_type":"code","source":["# import\n","torch.cuda.empty_cache()\n","from chatgpt.models.base import RewardModel\n","from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n","from chatgpt.models.gpt import GPTActor, GPTCritic\n","from chatgpt.models.opt import OPTActor, OPTCritic\n","from chatgpt.trainer import PPOTrainer\n","from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n","from torch.optim import Adam\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","\n","from colossalai.nn.optimizer import HybridAdam\n","\n","## clossalAI error 해결\n","os.environ['RANK'] = '0'\n","os.environ['LOCAL_RANK'] = '0'\n","os.environ['WORLD_SIZE'] = '2'\n","os.environ['MASTER_ADDR'] = '127.0.0.1'\n","os.environ['MASTER_PORT'] = '42043'\n","\n","# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n","        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n","    ),\n","    \"prompt_no_input\": (\n","        \"Below is an instruction that describes a task.\\n\"\n","        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n","    ),\n","}"],"metadata":{"id":"IvR8uvoZcJPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--data_path_3_PPO', type=str, default='./kochatgpt_dataset/kochatgpt_3_PPO.jsonl')\n","parser.add_argument('--output_dir', type=str, default='./output_3_PPO')\n","parser.add_argument('--strategy',\n","                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n","                    default='naive')\n","parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--pretrain', type=str, default=None)\n","parser.add_argument('--num_episodes', type=int, default=10)\n","parser.add_argument('--max_timesteps', type=int, default=3)\n","parser.add_argument('--update_timesteps', type=int, default=3)\n","parser.add_argument('--max_epochs', type=int, default=5)\n","parser.add_argument('--train_batch_size', type=int, default=8)\n","parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n","parser.add_argument('--max_length', type=int, default=250)\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.output_dir = './output_3_PPO'\n","args.pretrain = 'skt/kogpt2-base-v2'  # pretrained 모델 가져오기\n","\n","args.pretrain_actor = './output_1_SFT'  # SFT 모델 가져오기\n","args.pretrain_critic = './output_2_RM'  # RM 모델 가져오기\n","# args.pretrain_actor = args.pretrain\n","# args.pretrain_critic = args.pretrain\n","\n","args.num_episodes = 1\n","args.max_epochs   = 1\n","\n","print(args)\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)"],"metadata":{"id":"zZQTezoCdF-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure strategy\n","if args.strategy == 'naive':\n","    strategy = NaiveStrategy()\n","elif args.strategy == 'ddp':\n","    strategy = DDPStrategy()\n","elif args.strategy == 'colossalai_gemini':\n","    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n","elif args.strategy == 'colossalai_zero2':\n","    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n","else:\n","    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"],"metadata":{"id":"jI4VirtndHU3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure model, tokenizer\n","with strategy.model_init_context():\n","    if args.model == 'gpt2':\n","        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","        # tokenizer.pad_token = tokenizer.eos_token\n","        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n","        tokenizer.add_special_tokens(\n","            {\n","                \"eos_token\": DEFAULT_EOS_TOKEN,\n","                \"bos_token\": DEFAULT_BOS_TOKEN,\n","                \"unk_token\": DEFAULT_UNK_TOKEN,\n","            }\n","        )\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    elif args.model == 'bloom':\n","        actor = BLOOMActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        critic = BLOOMCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n","        tokenizer.pad_token = tokenizer.eos_token\n","    elif args.model == 'opt':\n","        actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        critic = OPTCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n","    else:\n","        raise ValueError(f'Unsupported model \"{args.model}\"')\n","\n","    initial_model = deepcopy(actor)\n","    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"],"metadata":{"id":"T47TCyZVdIfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure optimizer\n","if args.strategy.startswith('colossalai'):\n","    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n","    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n","else:\n","    actor_optim = Adam(actor.parameters(), lr=5e-6)\n","    critic_optim = Adam(critic.parameters(), lr=5e-6)"],"metadata":{"id":"zJV34DhHdJ22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setting the models\n","(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n","    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"],"metadata":{"id":"ebH7NYWzdK5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare data\n","with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n","    list_data_dict = json.load(json_file)\n","    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n","\n","def tokenize_fn(texts):\n","    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n","    return {k: v.cuda() for k, v in batch.items()}\n","\n","print(list_prompt)\n","print('\\n\\n\\n')\n","print(tokenize_fn('I want you to act as a linux terminal.'))"],"metadata":{"id":"RE2E3nkJdpx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure trainer\n","trainer = PPOTrainer(strategy,\n","                     actor,\n","                     critic,\n","                     reward_model,\n","                     initial_model,\n","                     actor_optim,\n","                     critic_optim,\n","                     max_epochs=args.max_epochs,\n","                     train_batch_size=args.train_batch_size,\n","                     tokenizer=tokenize_fn,\n","                     max_length=128,\n","                     do_sample=True,\n","                     temperature=1.0,\n","                     top_k=50,\n","                     pad_token_id=tokenizer.pad_token_id,\n","                     eos_token_id=tokenizer.eos_token_id)\n","\n","## train!\n","trainer.fit(list_prompt,  # 입력 prompt\n","            num_episodes=args.num_episodes,\n","            max_timesteps=args.max_timesteps,\n","            update_timesteps=args.update_timesteps)\n","\n","## save\n","# save model checkpoint after fitting on only rank0\n","strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n","# save optimizer checkpoint on all ranks\n","strategy.save_optimizer(actor_optim,\n","                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n","                        only_rank0=False)"],"metadata":{"id":"yI4Cos_2dqc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## inference\n","def generation(input_text):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n","        torch.cuda.current_device())\n","    outputs = actor.generate(input_ids,\n","                             max_length=args.max_length,\n","                             do_sample=True,\n","                             top_k=50,\n","                             top_p=0.95,\n","                             num_return_sequences=1)\n","    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n","    print('#' * 70)\n","    print(output)\n","    return output\n","\n","\n","list_prompt = [\n","    '불고기용 고기 한우에요?',\n","    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n","    '시카고 오헤어 국제공항은 어디에 있어',\n","    '오늘 미세먼지 어때?']\n","\n","list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n","\n","for input_text in list_prompt:\n","    output = generation(input_text)"],"metadata":{"id":"VSzpNjdwdtbo"},"execution_count":null,"outputs":[]}]}
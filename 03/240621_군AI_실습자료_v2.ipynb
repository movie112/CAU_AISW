{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 240621(금) 중앙대학교 군 장병 AISW 역량강화: 고급자연어처리 실습 자료입니다.\n",
        "- 본 내용은 IIPL (Intelligent Information Processing Lab) 소속 석사과정 김영화 조교가 작성하였습니다.\n",
        "\n",
        "---\n",
        "## 03\n",
        "- DPO (Direct Preference Optimization)\n",
        "- [ref](https://devocean.sk.com/blog/techBoardDetail.do?ID=165903&boardType=techBlog#none)\n",
        "---"
      ],
      "metadata": {
        "id": "QG7hYeCXqjCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting\n",
        "### 라이브러리 설치"
      ],
      "metadata": {
        "id": "jZ1eqiXhrZTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q -U bitsandbytes==0.42.0\n",
        "!pip install -q -U transformers==4.38.2\n",
        "!pip3 install -q -U peft==0.9.0\n",
        "!pip3 install -q -U accelerate==0.27.2\n",
        "!pip3 install -q -U datasets==2.18.0\n",
        "!pip3 install -q -U trl==0.7.11"
      ],
      "metadata": {
        "id": "8XZT2MqWrZm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatML (Chat Markup Languague)"
      ],
      "metadata": {
        "id": "I6EJPgSotAJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  ChatML Prompt\n",
        "- OpenAI에서 대화 인터페이스를 효과적으로 관리 할 수 있도록 데이터의 구조를 나타내는 구문\n",
        "- ChatML prompt 형식\n",
        "```json\n",
        "<|im_start|>system\n",
        "모델의 초기 지침 사항\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "사용자의 메시지\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "```\n",
        "  - 문장의 시작은 <|im_start|> 로 시작, 이후 역할 (System, User, Assistant) 을 명시하고 문장의 끝은 <|im_end|> 구분자 토큰\n",
        "\n",
        "  1. 시스템 메시지로 모델의 초기 지침 사항을 설명하며, 모델이 사용자의 질문에 어떻게 반응해야 할지에 대한 지침이나 규칙등을 명시.\n",
        "\n",
        "  2. 사용자 메시지로 모델에게 질문할 내용\n",
        "\n",
        "  3. 모델이 응답할 차례임을 나타내는 <|im_start|>assistant 토큰"
      ],
      "metadata": {
        "id": "GmHzN-G9tHfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma for ChatML\n",
        "- https://huggingface.co/google/gemma-1.1-2b-it\n",
        "  - Chat template: Chat prompt 형식\n",
        "  ```json\n",
        "  <bos><start_of_turn>user\n",
        "Write a hello world program<end_of_turn>\n",
        "<start_of_turn>model\n",
        "```"
      ],
      "metadata": {
        "id": "-PvdQRX6utHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Load\n",
        "- https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6"
      ],
      "metadata": {
        "id": "hYntdKRrv622"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v0.6\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map={\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_special_tokens=True)"
      ],
      "metadata": {
        "id": "HMwb1HRtWyCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- special tokens 확인"
      ],
      "metadata": {
        "id": "QUsd00RgtAmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Special Tokens:\", tokenizer.special_tokens_map)"
      ],
      "metadata": {
        "id": "yTLUHYIQu51g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat 형식으로 대화"
      ],
      "metadata": {
        "id": "WxvKMkkm0KEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Why do I hate summer?\""
      ],
      "metadata": {
        "id": "nAqfM1Pau5zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"<bos><start_of_turn>system\n",
        "You are a helpful AI assistant.<end_of_turn>\n",
        "<start_of_turn>user\n",
        "{question}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GqJUm_HxtAuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 출력\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
        "outputs = pipe(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "KRtnHriJ0PTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat template 사용\n",
        "- HF tokenizer는 ChaML 형식을 만들 수 있는 템플릿을 제공\n",
        "  - https://huggingface.co/docs/transformers/main/en/chat_templating"
      ],
      "metadata": {
        "id": "PYOeSk6e0cSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"Which country's capital is Seoul?\" },\n",
        "    { \"role\": \"assistant\", \"content\": \"Seoul is the capital of Korea.\" },\n",
        "    { \"role\": \"user\", \"content\": \"How many people live in Seoul?\" },\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "3LXLlajt0qvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatML function\n",
        "- multiturn 형식으로 chatbot 처럼 대화할 수 있도록 prompt를 만들어 주는 함수 생성"
      ],
      "metadata": {
        "id": "a5u9ZXFa0v_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "def chat_func(input):\n",
        "    messages.append({\"role\": \"user\", \"content\": input})\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    print(\"prompt:\", prompt)\n",
        "\n",
        "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=256)\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    decoded_output = decoded_output.replace(\"<eos>\", \"\").strip()\n",
        "    parts = decoded_output.split('<start_of_turn>model')\n",
        "    last_output = parts[-1]\n",
        "    print(last_output)\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": last_output})"
      ],
      "metadata": {
        "id": "7TtqcQoz02-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인 코드\n",
        "chat_func(\"Why do I like summer?\")"
      ],
      "metadata": {
        "id": "zdx0OZSK0586"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_func(\"How can you feel the beauty of nature?\")"
      ],
      "metadata": {
        "id": "h44-M5iD1FGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DPO"
      ],
      "metadata": {
        "id": "w4stlvvr1fPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Load\n",
        "- https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1"
      ],
      "metadata": {
        "id": "rBqZKO9yrFQy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1v31-Ashl1g"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"jondurbin/truthy-dpo-v0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "F7SmBcqtrPTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][200]"
      ],
      "metadata": {
        "id": "xc6pU8Hzr5Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습용 프롬프트 조정\n",
        "- TinyLlama special token을 통해 chatml 형식으로 만듦"
      ],
      "metadata": {
        "id": "eS7r2eGH1mRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(example):\n",
        "    prompt = example['prompt']\n",
        "    rejected = example['rejected']\n",
        "    chosen = example['chosen']\n",
        "\n",
        "    example['prompt'] = f\"<bos><start_of_turn>system\\n <end_of_turn><start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    example['rejected'] = f\"{rejected}<end_of_turn>\\n<eos>\"\n",
        "    example['chosen'] = f\"{chosen}<end_of_turn>\\n<eos>\"\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "n5OUX9Vrr6UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_dataset = dataset.map(generate_prompt)"
      ],
      "metadata": {
        "id": "clmVop4r1qs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인\n",
        "transformed_dataset['train'][0]"
      ],
      "metadata": {
        "id": "Wf3sc5op1sQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data split\n",
        "dataset = transformed_dataset['train'].train_test_split(test_size=0.05)"
      ],
      "metadata": {
        "id": "7L0Qiijx18AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "ZQy8c4cK2AJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DPO 학습\n",
        "-  Colab에서 학습 할 수 있도록 QLoRa를 활용해 모델을 올리고 DPO 학습을 진행"
      ],
      "metadata": {
        "id": "meyNogKW2EYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "i1pHi0VJ2B7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 load\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v0.6\"\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\", quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'"
      ],
      "metadata": {
        "id": "ZtVI39lV2Rsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DPO Trainer 실행"
      ],
      "metadata": {
        "id": "MdEMlZbg25Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Arguments 설정\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=\"./outputs\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        do_eval=True,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=1,\n",
        "        per_device_eval_batch_size=2,\n",
        "        logging_steps=100,\n",
        "        learning_rate=5e-7,\n",
        "        eval_steps=100,\n",
        "        num_train_epochs=1,\n",
        "        save_steps=500,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        ")"
      ],
      "metadata": {
        "id": "mpp83vhG2S4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DPOTrainer\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    beta=0.1,\n",
        "    peft_config=lora_config,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    max_prompt_length=512,\n",
        "    max_length=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "t3ZVkQ6C2_VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습: 약 25분\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "collapsed": true,
        "id": "rpESxwul3MGN",
        "outputId": "e5ad69f6-29e9-4356-fcb5-d43728feee9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='965' max='965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [965/965 23:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rewards/chosen</th>\n",
              "      <th>Rewards/rejected</th>\n",
              "      <th>Rewards/accuracies</th>\n",
              "      <th>Rewards/margins</th>\n",
              "      <th>Logps/rejected</th>\n",
              "      <th>Logps/chosen</th>\n",
              "      <th>Logits/rejected</th>\n",
              "      <th>Logits/chosen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.692700</td>\n",
              "      <td>0.691564</td>\n",
              "      <td>0.003891</td>\n",
              "      <td>0.000709</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.003182</td>\n",
              "      <td>-186.024521</td>\n",
              "      <td>-154.832626</td>\n",
              "      <td>-4.003264</td>\n",
              "      <td>-3.812699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.690100</td>\n",
              "      <td>0.688496</td>\n",
              "      <td>0.011521</td>\n",
              "      <td>0.002155</td>\n",
              "      <td>0.865385</td>\n",
              "      <td>0.009365</td>\n",
              "      <td>-186.010071</td>\n",
              "      <td>-154.756332</td>\n",
              "      <td>-4.003408</td>\n",
              "      <td>-3.812675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.686800</td>\n",
              "      <td>0.685275</td>\n",
              "      <td>0.018820</td>\n",
              "      <td>0.002965</td>\n",
              "      <td>0.865385</td>\n",
              "      <td>0.015855</td>\n",
              "      <td>-186.001953</td>\n",
              "      <td>-154.683334</td>\n",
              "      <td>-4.003627</td>\n",
              "      <td>-3.812690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.682900</td>\n",
              "      <td>0.682241</td>\n",
              "      <td>0.027336</td>\n",
              "      <td>0.005342</td>\n",
              "      <td>0.865385</td>\n",
              "      <td>0.021994</td>\n",
              "      <td>-185.978195</td>\n",
              "      <td>-154.598175</td>\n",
              "      <td>-4.003800</td>\n",
              "      <td>-3.812740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.681100</td>\n",
              "      <td>0.679886</td>\n",
              "      <td>0.032155</td>\n",
              "      <td>0.005354</td>\n",
              "      <td>0.884615</td>\n",
              "      <td>0.026801</td>\n",
              "      <td>-185.978104</td>\n",
              "      <td>-154.549988</td>\n",
              "      <td>-4.003926</td>\n",
              "      <td>-3.812731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.679200</td>\n",
              "      <td>0.678066</td>\n",
              "      <td>0.036169</td>\n",
              "      <td>0.005626</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.030543</td>\n",
              "      <td>-185.975357</td>\n",
              "      <td>-154.509857</td>\n",
              "      <td>-4.004046</td>\n",
              "      <td>-3.812742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.677300</td>\n",
              "      <td>0.676949</td>\n",
              "      <td>0.038598</td>\n",
              "      <td>0.005755</td>\n",
              "      <td>0.865385</td>\n",
              "      <td>0.032844</td>\n",
              "      <td>-185.974060</td>\n",
              "      <td>-154.485535</td>\n",
              "      <td>-4.004115</td>\n",
              "      <td>-3.812745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.676309</td>\n",
              "      <td>0.040360</td>\n",
              "      <td>0.006190</td>\n",
              "      <td>0.865385</td>\n",
              "      <td>0.034170</td>\n",
              "      <td>-185.969727</td>\n",
              "      <td>-154.467941</td>\n",
              "      <td>-4.004153</td>\n",
              "      <td>-3.812748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.677500</td>\n",
              "      <td>0.676144</td>\n",
              "      <td>0.040724</td>\n",
              "      <td>0.006212</td>\n",
              "      <td>0.865385</td>\n",
              "      <td>0.034512</td>\n",
              "      <td>-185.969482</td>\n",
              "      <td>-154.464294</td>\n",
              "      <td>-4.004165</td>\n",
              "      <td>-3.812750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=965, training_loss=0.6821798848364637, metrics={'train_runtime': 1395.6389, 'train_samples_per_second': 0.691, 'train_steps_per_second': 0.691, 'total_flos': 0.0, 'train_loss': 0.6821798848364637, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습된 lora weight 저장"
      ],
      "metadata": {
        "id": "N2uTMplz1Hf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ADAPTER_MODEL = \"lora_adapter\"\n",
        "\n",
        "trainer.model.save_pretrained(ADAPTER_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5jX7jzNomqw",
        "outputId": "54326706-c64a-49a9-c2e0-d8957b87ea11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 하나의 fine-tuned model"
      ],
      "metadata": {
        "id": "iOAGhL5S1bUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
        "\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(\"./TinyLlama-1.1B-Chat-v0.6_DPO\") # Save the merged model"
      ],
      "metadata": {
        "id": "h9KMV7d5yPGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 추론"
      ],
      "metadata": {
        "id": "eZFLmMSI1zIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 저장한 모델 로드"
      ],
      "metadata": {
        "id": "7NEOIOa81oZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v0.6\"\n",
        "FINETUNE_MODEL = \"./TinyLlama-1.1B-Chat-v0.6_DPO\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map={\"\":0})\n",
        "finetune_model = AutoModelForCausalLM.from_pretrained(FINETUNE_MODEL, device_map={\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_special_tokens=True)"
      ],
      "metadata": {
        "id": "SkC0lUaf1n-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_base = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer, max_new_tokens=512)"
      ],
      "metadata": {
        "id": "B5GMOAA44JNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_finetuned = pipeline(\"text-generation\", model=finetune_model, tokenizer=tokenizer, max_new_tokens=512)"
      ],
      "metadata": {
        "id": "uRQAGh6i0-wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset['test'][10]['prompt']"
      ],
      "metadata": {
        "id": "Tbby1wD916pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "W0Y7wqEU2VyC",
        "outputId": "09d5c5eb-6dc8-4cfa-dc44-3974a9a5031a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos><start_of_turn>system\\n <end_of_turn><start_of_turn>user\\nDoes microwaving food destroy its nutrients?<end_of_turn>\\n<start_of_turn>model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DPO 이전 모델 추론"
      ],
      "metadata": {
        "id": "MrzfWlo5hoi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe_base(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "RFfXmXir3s_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DPO 이후 모델 추론"
      ],
      "metadata": {
        "id": "7xRYcNYuhx2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe_finetuned(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "zxGPmxO63WLx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
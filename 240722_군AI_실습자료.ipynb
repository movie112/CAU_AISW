{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 240722(월) 중앙대학교 군 장병 AISW 역량강화: 고급자연어처리 실습 자료입니다.\n",
        "- 본 내용은 IIPL (Intelligent Information Processing Lab) 소속 석사과정 김영화 조교가 작성하였습니다.\n",
        "\n",
        "\n",
        "- Rewardbench 톺아보기\n",
        "- 감정분류 예제\n",
        "-----"
      ],
      "metadata": {
        "id": "TLd6vj-DRmHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rewardbench\n",
        "\n",
        "- dataset: https://huggingface.co/datasets/allenai/reward-bench\n",
        "- leaderboard: https://huggingface.co/spaces/allenai/reward-bench\n",
        "- github: https://github.com/allenai/reward-bench"
      ],
      "metadata": {
        "id": "u4it4VKgSDhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 감정분류 예제"
      ],
      "metadata": {
        "id": "YF9AV5F3SUyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# text processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# sklearn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# utils\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiOL3sX31CtR",
        "outputId": "79e4938f-beff-438a-876f-0a83410e9619"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "data = pd.read_csv('IMDB Dataset.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vyGy1VSN9ip",
        "outputId": "38314cd1-b0a2-4dc9-b3fe-6b00600c11d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_label(label):\n",
        "    return 1 if label == 'positive' else 0\n",
        "\n",
        "data['label'] = data['sentiment'].progress_apply(transform_label)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "nrJpTHd5dNvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_label(label):\n",
        "    return 1 if label == 'positive' else 0\n",
        "\n",
        "data['label'] = data['sentiment'].progress_apply(transform_label)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "SOuw4nlc4HHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sentiment.value_counts()"
      ],
      "metadata": {
        "id": "y6_nODd64HkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['token_length'] = data.review.progress_apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "0T3pb9XJ4PZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_pos = data[data['label'] == 1]\n",
        "data_pos['token_length'].describe()"
      ],
      "metadata": {
        "id": "BDrtcY6O4WVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_neg = data[data['label'] == 0]\n",
        "data_neg['token_length'].describe()"
      ],
      "metadata": {
        "id": "iZxfAheu4WT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 8))\n",
        "sns.displot(data_pos, x='token_length')\n",
        "plt.title('Positive Token Length Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q8R_VvVJ4WR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 8))\n",
        "sns.displot(data_pos, x='token_length')\n",
        "plt.title('Negative Token Length Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "athHj6hz4WPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see most minimum length token\n",
        "print('Positive')\n",
        "print(data_pos[data_pos['token_length'] == data_pos['token_length'].min()]['review'].item())\n",
        "print()\n",
        "print('Negative')\n",
        "print(data_neg[data_neg['token_length'] == data_neg['token_length'].min()]['review'].item())"
      ],
      "metadata": {
        "id": "pMGHCK2p4WNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text cleaning\n",
        "\n",
        "def rm_link(text):\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "# handle case like \"shut up okay?Im only 10 years old\"\n",
        "# become \"shut up okay Im only 10 years old\"\n",
        "def rm_punct2(text):\n",
        "    # return re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
        "    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
        "\n",
        "def rm_html(text):\n",
        "    return re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "def space_bt_punct(text):\n",
        "    pattern = r'([.,!?-])'\n",
        "    s = re.sub(pattern, r' \\1 ', text)     # add whitespaces between punctuation\n",
        "    s = re.sub(r'\\s{2,}', ' ', s)        # remove double whitespaces\n",
        "    return s\n",
        "\n",
        "def rm_number(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def rm_whitespaces(text):\n",
        "    return re.sub(r' +', ' ', text)\n",
        "\n",
        "def rm_nonascii(text):\n",
        "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
        "\n",
        "def rm_emoji(text):\n",
        "    emojis = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE\n",
        "    )\n",
        "    return emojis.sub(r'', text)\n",
        "\n",
        "def spell_correction(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "\n",
        "def clean_pipeline(text):\n",
        "    no_link = rm_link(text)\n",
        "    no_html = rm_html(no_link)\n",
        "    space_punct = space_bt_punct(no_html)\n",
        "    no_punct = rm_punct2(space_punct)\n",
        "    no_number = rm_number(no_punct)\n",
        "    no_whitespaces = rm_whitespaces(no_number)\n",
        "    no_nonasci = rm_nonascii(no_whitespaces)\n",
        "    no_emoji = rm_emoji(no_nonasci)\n",
        "    spell_corrected = spell_correction(no_emoji)\n",
        "    return spell_corrected"
      ],
      "metadata": {
        "id": "iDww1hxB4WK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def rm_stopwords(text):\n",
        "    return [i for i in text if i not in stopwords]\n",
        "\n",
        "def lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
        "    # make sure lemmas does not contains sotpwords\n",
        "    return rm_stopwords(lemmas)\n",
        "\n",
        "def preprocess_pipeline(text):\n",
        "    tokens = tokenize(text)\n",
        "    no_stopwords = rm_stopwords(tokens)\n",
        "    lemmas = lemmatize(no_stopwords)\n",
        "    return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "yV0X2Krq4jwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean'] = data['review'].progress_apply(clean_pipeline)\n",
        "data['processed'] = data['clean'].progress_apply(preprocess_pipeline)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "7Rt9FWxo4kxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keep only processed and label columns\n",
        "data[['processed', 'label']].to_csv('./imdb_processed.csv', index=False, header=True)"
      ],
      "metadata": {
        "id": "68N8USM54mIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read processed data\n",
        "data = pd.read_csv('./imdb_processed.csv')\n",
        "\n",
        "for row in data[:2].iterrows():\n",
        "    print(row[1]['processed'])\n",
        "    print(f'Label: {row[1][\"label\"]}')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "gYM4brdG4mqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = data.processed.values\n",
        "# merge into single variable, separated by whitespaces\n",
        "words = ' '.join(reviews)\n",
        "# obtain list of words\n",
        "words = words.split()\n",
        "\n",
        "# check our list\n",
        "words[:10]"
      ],
      "metadata": {
        "id": "6hEg2Z9c4nwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocabulary\n",
        "counter = Counter(words)\n",
        "vocab = sorted(counter, key=counter.get, reverse=True)\n",
        "int2word = dict(enumerate(vocab, 1))\n",
        "int2word[0] = '<PAD>'\n",
        "word2int = {word: id for id, word in int2word.items()}"
      ],
      "metadata": {
        "id": "hOxvau444rq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode words\n",
        "reviews_enc = [[word2int[word] for word in review.split()] for review in tqdm(reviews)]\n",
        "\n",
        "# print first-10 words of first 5 reviews\n",
        "for i in range(5):\n",
        "    print(reviews_enc[i][:5])"
      ],
      "metadata": {
        "id": "z3CqIT1s4tNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding sequences\n",
        "\n",
        "def pad_features(reviews, pad_id, seq_length=128):\n",
        "    # features = np.zeros((len(reviews), seq_length), dtype=int)\n",
        "    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n",
        "\n",
        "    for i, row in enumerate(reviews):\n",
        "        # if seq_length < len(row) then review will be trimmed\n",
        "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
        "\n",
        "    return features\n",
        "\n",
        "seq_length = 256\n",
        "features = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)\n",
        "\n",
        "assert len(features) == len(reviews_enc)\n",
        "assert len(features[0]) == seq_length\n",
        "\n",
        "features[:10, :10]"
      ],
      "metadata": {
        "id": "2LXcv32y4rpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get labels as numpy\n",
        "labels = data.label.to_numpy()\n",
        "labels"
      ],
      "metadata": {
        "id": "mM75SsOJ4rnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "train_size = .7     # we will use 80% of whole data as train set\n",
        "val_size = .5       # and we will use 50% of test set as validation set\n",
        "\n",
        "# make train set\n",
        "split_id = int(len(features) * train_size)\n",
        "train_x, remain_x = features[:split_id], features[split_id:]\n",
        "train_y, remain_y = labels[:split_id], labels[split_id:]\n",
        "\n",
        "# make val and test set\n",
        "split_val_id = int(len(remain_x) * val_size)\n",
        "val_x, test_x = remain_x[:split_val_id], remain_x[split_val_id:]\n",
        "val_y, test_y = remain_y[:split_val_id], remain_y[split_val_id:]"
      ],
      "metadata": {
        "id": "HryDCR4p4rlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the shape\n",
        "print('Feature Shapes:')\n",
        "print('===============')\n",
        "print('Train set: {}'.format(train_x.shape))\n",
        "print('Validation set: {}'.format(val_x.shape))\n",
        "print('Test set: {}'.format(test_x.shape))"
      ],
      "metadata": {
        "id": "BthNnbpR4rjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_y[train_y == 0]), len(train_y[train_y == 1]))\n",
        "print(len(val_y[val_y == 0]), len(val_y[val_y == 1]))\n",
        "print(len(test_y[test_y == 0]), len(test_y[test_y == 1]))"
      ],
      "metadata": {
        "id": "r_TvUzd_4rhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define batch size\n",
        "batch_size = 128\n",
        "\n",
        "# create tensor datasets\n",
        "trainset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "validset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "testset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# create dataloaders\n",
        "trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
        "valloader = DataLoader(validset, shuffle=True, batch_size=batch_size)\n",
        "testloader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "XlijMo6e4rd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check our batches\n",
        "dataiter = iter(trainloader)\n",
        "x, y = dataiter.next()\n",
        "\n",
        "print('Sample batch size: ', x.size())   # batch_size, seq_length\n",
        "print('Sample batch input: \\n', x)\n",
        "print()\n",
        "print('Sample label size: ', y.size())   # batch_size\n",
        "print('Sample label input: \\n', y)"
      ],
      "metadata": {
        "id": "EsDJKPlY47Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model architecture\n",
        "\n",
        "class SentimentModel(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, hidden_size=128, embedding_size=400, n_layers=2, dropout=0.2):\n",
        "        super(SentimentModel, self).__init__()\n",
        "\n",
        "        # embedding layer is useful to map input into vector representation\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "        # LSTM layer preserved by PyTorch library\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # Linear layer for output\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Sigmoid layer cz we will have binary classification\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # convert feature to long\n",
        "        x = x.long()\n",
        "\n",
        "        # map input to vector\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # pass forward to lstm\n",
        "        o, _ =  self.lstm(x)\n",
        "\n",
        "        # get last sequence output\n",
        "        o = o[:, -1, :]\n",
        "\n",
        "        # apply dropout and fully connected layer\n",
        "        o = self.dropout(o)\n",
        "        o = self.fc(o)\n",
        "\n",
        "        # sigmoid\n",
        "        o = self.sigmoid(o)\n",
        "        return o"
      ],
      "metadata": {
        "id": "t5venZwX4-X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "7C2su8Zl4_sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model hyperparamters\n",
        "vocab_size = len(word2int)\n",
        "output_size = 1\n",
        "embedding_size = 256\n",
        "hidden_size = 512\n",
        "n_layers = 2\n",
        "dropout=0.25\n",
        "\n",
        "# model initialization\n",
        "model = SentimentModel(vocab_size, output_size, hidden_size, embedding_size, n_layers, dropout)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "xwY0oxgG4_rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training config\n",
        "lr = 0.001\n",
        "criterion = nn.BCELoss()  # we use BCELoss cz we have binary classification problem\n",
        "optim = Adam(model.parameters(), lr=lr)\n",
        "grad_clip = 5\n",
        "epochs = 8\n",
        "print_every = 1\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': [],\n",
        "    'epochs': epochs\n",
        "}\n",
        "es_limit = 5"
      ],
      "metadata": {
        "id": "RcOVYsH64-WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train loop\n",
        "model = model.to(device)\n",
        "\n",
        "epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n",
        "\n",
        "# early stop trigger\n",
        "es_trigger = 0\n",
        "val_loss_min = torch.inf\n",
        "\n",
        "for e in epochloop:\n",
        "\n",
        "    #################\n",
        "    # training mode #\n",
        "    #################\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    for id, (feature, target) in enumerate(trainloader):\n",
        "        # add epoch meta info\n",
        "        epochloop.set_postfix_str(f'Training batch {id}/{len(trainloader)}')\n",
        "\n",
        "        # move to device\n",
        "        feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "        # reset optimizer\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        out = model(feature)\n",
        "\n",
        "        # acc\n",
        "        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
        "        equals = predicted == target\n",
        "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
        "        train_acc += acc.item()\n",
        "\n",
        "        # loss\n",
        "        loss = criterion(out.squeeze(), target.float())\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        # clip grad\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "        # update optimizer\n",
        "        optim.step()\n",
        "\n",
        "        # free some memory\n",
        "        del feature, target, predicted\n",
        "\n",
        "    history['train_loss'].append(train_loss / len(trainloader))\n",
        "    history['train_acc'].append(train_acc / len(trainloader))\n",
        "\n",
        "    ####################\n",
        "    # validation model #\n",
        "    ####################\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for id, (feature, target) in enumerate(valloader):\n",
        "            # add epoch meta info\n",
        "            epochloop.set_postfix_str(f'Validation batch {id}/{len(valloader)}')\n",
        "\n",
        "            # move to device\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            out = model(feature)\n",
        "\n",
        "            # acc\n",
        "            predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
        "            equals = predicted == target\n",
        "            acc = torch.mean(equals.type(torch.FloatTensor))\n",
        "            val_acc += acc.item()\n",
        "\n",
        "            # loss\n",
        "            loss = criterion(out.squeeze(), target.float())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # free some memory\n",
        "            del feature, target, predicted\n",
        "\n",
        "        history['val_loss'].append(val_loss / len(valloader))\n",
        "        history['val_acc'].append(val_acc / len(valloader))\n",
        "\n",
        "    # reset model mode\n",
        "    model.train()\n",
        "\n",
        "    # add epoch meta info\n",
        "    epochloop.set_postfix_str(f'Val Loss: {val_loss / len(valloader):.3f} | Val Acc: {val_acc / len(valloader):.3f}')\n",
        "\n",
        "    # print epoch\n",
        "    if (e+1) % print_every == 0:\n",
        "        epochloop.write(f'Epoch {e+1}/{epochs} | Train Loss: {train_loss / len(trainloader):.3f} Train Acc: {train_acc / len(trainloader):.3f} | Val Loss: {val_loss / len(valloader):.3f} Val Acc: {val_acc / len(valloader):.3f}')\n",
        "        epochloop.update()\n",
        "\n",
        "    # save model if validation loss decrease\n",
        "    if val_loss / len(valloader) <= val_loss_min:\n",
        "        torch.save(model.state_dict(), './sentiment_lstm.pt')\n",
        "        val_loss_min = val_loss / len(valloader)\n",
        "        es_trigger = 0\n",
        "    else:\n",
        "        epochloop.write(f'[WARNING] Validation loss did not improved ({val_loss_min:.3f} --> {val_loss / len(valloader):.3f})')\n",
        "        es_trigger += 1\n",
        "\n",
        "    # force early stop\n",
        "    if es_trigger >= es_limit:\n",
        "        epochloop.write(f'Early stopped at Epoch-{e+1}')\n",
        "        # update epochs history\n",
        "        history['epochs'] = e+1\n",
        "        break"
      ],
      "metadata": {
        "id": "8cbRCeG64-T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss\n",
        "plt.figure(figsize=(6, 8))\n",
        "plt.plot(range(history['epochs']), history['train_acc'], label='Train Acc')\n",
        "plt.plot(range(history['epochs']), history['val_acc'], label='Val Acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8MuWlzsE4-R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss\n",
        "plt.figure(figsize=(6, 8))\n",
        "plt.plot(range(history['epochs']), history['train_loss'], label='Train Loss')\n",
        "plt.plot(range(history['epochs']), history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kD84m_lx5Tq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test loop\n",
        "model.eval()\n",
        "\n",
        "# metrics\n",
        "test_loss = 0\n",
        "test_acc = 0\n",
        "\n",
        "all_target = []\n",
        "all_predicted = []\n",
        "\n",
        "testloop = tqdm(testloader, leave=True, desc='Inference')\n",
        "with torch.no_grad():\n",
        "    for feature, target in testloop:\n",
        "        feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "        out = model(feature)\n",
        "\n",
        "        predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
        "        equals = predicted == target\n",
        "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
        "        test_acc += acc.item()\n",
        "\n",
        "        loss = criterion(out.squeeze(), target.float())\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        all_target.extend(target.cpu().numpy())\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "\n",
        "    print(f'Accuracy: {test_acc/len(testloader):.4f}, Loss: {test_loss/len(testloader):.4f}')"
      ],
      "metadata": {
        "id": "Y9lx7X_O5Ynk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(all_predicted, all_target))"
      ],
      "metadata": {
        "id": "XmgCNVLo5YlB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}